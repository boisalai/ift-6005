# Rapport de mi-session

> Version du 1er mars 2025

## 1. Introduction

L'acc√®s aux informations nutritionnelles reste souvent limit√© par des interfaces techniques n√©cessitant des comp√©tences en langages de requ√™te comme SQL. Cette barri√®re emp√™che de nombreux utilisateurs d'exploiter pleinement des bases de donn√©es comme Open Food Facts, qui contient des informations d√©taill√©es sur des milliers de produits alimentaires. 

Ce projet vise √† d√©velopper un agent conversationnel utilisant des grands mod√®les de langage (LLM) pour permettre aux utilisateurs de poser des questions en langage naturel comme "Quelles collations sans allerg√®nes ont un Nutri-score A ?". Cette approche d√©mocratise l'acc√®s aux donn√©es nutritionnelles tout en am√©liorant la qualit√© des r√©ponses gr√¢ce √† l'exploitation directe de sources structur√©es. 

Ce rapport de mi-session pr√©sente l'√©tat d'avancement du projet √† la mi-session, les d√©fis rencontr√©s et les solutions impl√©ment√©es.

## 2. Rappel de l'objectif du projet

L'objectif de ce projet est de d√©velopper un agent conversationnel permettant aux utilisateurs d'interroger la base de donn√©es [Open Food Facts](https://world.openfoodfacts.org/) en langage naturel. Le syst√®me doit comprendre les questions des utilisateurs sur les produits alimentaires, les convertir en requ√™tes SQL, et fournir des r√©ponses claires et pr√©cises. Les donn√©es manquantes ou incompl√®tes doivent √™tre compens√©es par une recherche alternative dans le [Guide alimentaire canadien](https://guide-alimentaire.canada.ca/fr/).

## 3. Approche propos√©e

### 3.1 Architecture du syst√®me

Mon approche utilise une architecture avec les composants suivants :

- **Module de dialogue**: G√®re les conversations avec l'utilisateur en utilisant un LLM pr√©-entra√Æn√©.
- **Convertisseur texte-SQL**: Transforme les questions en requ√™tes SQL adapt√©es √† Open Food Facts. Il utilise d'abord une recherche dans un dictionnaire de donn√©es pour trouver les colonnes pertinentes.
- **Connecteur de base de donn√©es**: Communique avec DuckDB pour ex√©cuter les requ√™tes. Les requ√™tes SQL sont v√©rifi√©es avant ex√©cution pour garantir leur s√©curit√©.
- **Recherche sur le Web**: Consulte le [Guide alimentaire canadien](https://guide-alimentaire.canada.ca/fr/) quand les informations manquent dans la base de donn√©es.
- **G√©n√©rateur de r√©ponses**: Transforme les r√©sultats bruts en r√©ponses naturelles, en pr√©cisant les sources.

### 3.2 S√©lection des technologies

J'ai choisi ces technologies pour mon projet :

- **DuckDB** comme base de donn√©es, car :
  - Elle est rapide pour les requ√™tes analytiques
  - Elle g√®re bien la m√©moire
  - Elle supporte les fichiers Parquet et les requ√™tes SQL complexes
  - Elle s'int√®gre facilement avec Python
- **Smolagents de Hugging Face** comme framework d'agent car :
  - Il est simple √† utiliser
  - Il est con√ßu pour les "agents de code"
  - Il permet d'int√©grer diff√©rents LLMs et outils
  - Il est bien document√© et support√©
- **Mod√®les de langage** :
  - Pour le d√©veloppement: `ollama/llama3.1:8b-instruct-q8_0` (mod√®le local)
  - Pour les tests: `ollama/llama3.1:8b-instruct-q8_0`, `ollama/qwen2.5:7b-instruct` et `anthropic/claude-3-5-sonnet` (mod√®le commercial)
  - Cette approche limite les co√ªts d'API
- **FAISS** pour la recherche s√©mantique :
  - Il identifie rapidement les colonnes pertinentes pour chaque question
  - Il transforme la documentation des colonnes en vecteurs faciles √† comparer

## 4. √âtat d'avancement et t√¢ches r√©alis√©es

Le d√©veloppement de l'agent conversationnel a progress√© significativement durant cette premi√®re phase du projet. Cette section d√©taille les diff√©rentes t√¢ches accomplies et leur √©tat d'avancement, en suivant le plan initial.

### 4.1 Mise en place de l'environnement de d√©veloppement

J'ai d'abord cr√©√© un environnement de d√©veloppement complet:

- **Infrastructure et outils**
  - Un [d√©p√¥t GitHub](https://github.com/boisalai/ift-6005) pour le code et la documentation
  - Un environnement virtuel Python avec les bonnes d√©pendances
  - Des outils comme Black pour le formatage et Pylint pour l'analyse de code
  - Des hooks pre-commit pour maintenir la qualit√© du code
- **D√©pendances principales**
  - DuckDB (v1.2.0) pour les requ√™tes SQL
  - Smolagents (v1.9.2) comme framework d'agent
  - FAISS (v1.10.0) pour la recherche s√©mantique
  - Des biblioth√®ques pour l'analyse de donn√©es et le traitement du langage
- **Configuration des API**
  - Des variables d'environnement pour les cl√©s API dans un fichier `.env`
  - L'acc√®s √† l'API d'Anthropic pour Claude 3.5 Sonnet
  - L'interface avec Ollama pour les tests locaux


### 4.2 Pr√©paration de la base de donn√©es

J'ai t√©l√©charg√© le fichier Parquet d'Open Food Facts (3,6 millions de produits) et l'ai converti en base DuckDB:

```bash
wget -P data/ https://huggingface.co/datasets/openfoodfacts/product-database/resolve/main/food.parquet
```

```python
DATA_DIR = Path("../data")
PARQUET_PATH = DATA_DIR / "food.parquet"
FULL_DB_PATH = DATA_DIR / "food_full.duckdb"

con = duckdb.connect(str(FULL_DB_PATH), config={'memory_limit': '8GB'})
con.execute(f"CREATE TABLE products AS SELECT * FROM '{PARQUET_PATH}'")
con.close()
```

Pour acc√©l√©rer les requ√™tes, j'ai cr√©√© une version avec seulement les 94 802 produits canadiens :

```python
FILTERED_DB_PATH = DATA_DIR / "food_canada.duckdb"

con = duckdb.connect(str(FILTERED_DB_PATH))
con.execute(f"ATTACH DATABASE '{FULL_DB_PATH}' AS full_db")
con.execute(f"""
    CREATE TABLE products AS 
    SELECT * FROM full_db.products
    WHERE array_contains(countries_tags, 'en:canada')
""")
```

J'ai aussi analys√© les donn√©es et d√©couvert que certaines colonnes sont tr√®s compl√®tes (>95%) alors que d'autres sont peu renseign√©es (<30%).

![missing-values](../img/missing_values.png)

J'ai √©galement remarqu√© que certaines colonnes ont des structures complexes. 

Par exemple, "categories" contient du texte libre s√©par√© par des virgules, alors que "categories_tags" contient des identifiants standardis√©s incluant des pr√©fixes de langue.

Exemple de donn√©es pour la colonne "categories" :

```
"Sweeteners,Syrups,Simple syrups,Agave syrups"
"Plant-based beverages,Fruit-based,Juices and nectars,Fruit juices,Lemon juice"
"Snacks,Sweet snacks,Biscuits and cakes,Cakes"
```

Exemple de donn√©es pour la colonne "categories_tags" :

```
["en:plant-based-foods-and-beverages", "en:beverages", "en:plant-based-beverages", "en:coconut-milks"]
["en:snacks", "en:sweet-snacks", "en:cocoa-and-chocolate-products", "en:chocolates"]
["en:dairies", "en:milk", "en:whole-milk"]
```

Ces t√¢ches s'effectuent via le script `data.py`.

### 4.3 Documentation des donn√©es

J'ai cr√©√© un dictionnaire d√©taill√© en JSON (`columns_documentation.json`) pour aider l'agent √† g√©n√©rer des requ√™tes SQL. Il contient des informations sur chacune des 109 colonnes.
La documentation pour chaque colonne ressemble √† ceci:

```python
"nova_groups_tags": {
    "type": "VARCHAR[]",
    "description": "Array containing NOVA food classification group tags. NOVA...
    "examples": [
      "['en:1-unprocessed-or-minimally-processed-foods']",
      "['en:4-ultra-processed-food-and-drink-products']",
      "['en:3-processed-foods']"
    ],
    "is_nullable": true,
    "common_queries": [
    {
        "description": "Distribution of products across NOVA groups",
        "sql": "SELECT nova_groups_tags, COUNT(*) as product_count FROM products..."
    },
    {
        "description": "Find products in a specific NOVA group (e.g., ultra-pro...","
        "sql": "SELECT code, product_name, nova_groups_tags FROM products WHERE..."
    },
    {
        "description": "Products with unknown or missing NOVA classification",
        "sql": "SELECT code, product_name, nova_groups_tags FROM products WHERE..."
    }
  ]
},
```

Cette documentation est g√©n√©r√©e par un agent qui :

- Interroge la colonne dans la base de donn√©es
- Recherche des informations sur le site Open Food Facts
- Propose une description de la colonne
- G√©n√®re des requ√™tes SQL typiques
- Ajoute la documentation au fichier JSON

Ces t√¢ches s'effectuent via le script `docoff.py`.

### 4.4 Cr√©ation du jeu de test

J'ai cr√©√© un jeu de questions-r√©ponses en utilisant les requ√™tes SQL document√©es. Pour chaque requ√™te SQL, un agent:

√âvalue si la requ√™te r√©pond √† une question pertinente pour un consommateur
G√©n√®re des questions en fran√ßais et en anglais
Cr√©e des r√©ponses claires correspondant √† la requ√™te SQL
V√©rifie que les r√©sultats de la requ√™te permettent de r√©pondre √† la question

Les paires sont stock√©es dans `qa_pairs.json` avec cette structure:

```json
{
    "column": "nutriscore_grade",
    "sql": "SELECT code, product_name FROM products WHERE...",
    "questions": {
        "fr": "Quels produits ont un Nutri-Score A?",
        "en": "Which products have a Nutri-Score A?"
    },
    "answers": {
        "fr": "Les produits suivants ont un Nutri-Score A...",
        "en": "The following products have a Nutri-Score A..."
    }
}
# ... plus de paires Q&A
```

Le processus est impl√©ment√© dans le script `question_answer.py`.

### 4.5 D√©veloppement de la recherche s√©mantique

J'ai impl√©ment√© une recherche s√©mantique avec FAISS et le mod√®le `all-MiniLM-L6-v2` pour identifier les colonnes pertinentes dans la base de donn√©es :

- Pr√©paration des embeddings :
  - La documentation des 109 colonnes est convertie en vecteurs
  - Le mod√®le MiniLM g√©n√®re des embeddings de dimension 384
  - Ce mod√®le fonctionne en fran√ßais et en anglais
- Indexation FAISS :
  - Un index est cr√©√© pour les recherches rapides par similarit√©
  - Les embeddings sont sauvegard√©s dans des fichiers pour √©viter de recalculer l'index
- Processus de recherche :
  - Pour chaque question, l'embedding de la question est compar√© √† ceux des colonnes
  - Les 5 colonnes les plus similaires sont retourn√©es avec un score
  - Seules les colonnes avec un score > 0.5 sont utilis√©es

Les r√©sultats de cette recherche sont ajout√©s au prompt de l'agent pour l'aider √† g√©n√©rer de meilleures requ√™tes SQL.

### 4.6 Cr√©ation des outils d'agent

Un agent d'intelligence artificielle (IA) est un logiciel qui peut interagir avec son environnement, collecter des donn√©es et les utiliser pour effectuer des t√¢ches autod√©termin√©es afin d'atteindre des objectifs pr√©d√©termin√©s.

J'ai choisi Smolagents pour son approche simple et flexible.  Cet outil est id√©al pour exp√©rimenter rapidement avec la logique d'agent, 
particuli√®rement lorsque l'application est relativement simple.

J'ai d√©velopp√© deux outils principaux :

- **Ex√©cution SQL s√©curis√©e**: Permet √† l'agent de g√©n√©rer et d'ex√©cuter des requ√™tes SQL sur DuckDB avec des v√©rifications de s√©curit√©
- **Recherche sur le Web**: Permet √† l'agent de consulter le Guide alimentaire canadien quand les informations ne sont pas dans Open Food Facts

Ces outils sont orchestr√©s par Smolagents, qui aide l'agent √† choisir le bon outil selon la question.

### 4.7 Strat√©gie d'√©valuation

J'√©value l'agent avec quatre m√©triques principales:

- **Pr√©cision d'ex√©cution (EX)** : Mesure si l'agent g√©n√®re des requ√™tes SQL correctes (20% pour avoir une requ√™te, 30% pour l'ex√©cution sans erreur, 50% pour des r√©sultats corrects)
- **Pr√©cision s√©mantique (PS)** : Compare la similarit√© entre les r√©ponses de l'agent et les r√©ponses attendues
- **Respect de s√©quence (RS)** : V√©rifie si l'agent suit une strat√©gie de recherche coh√©rente (d'abord la base de donn√©es, puis les sources externes)
- **Temps de r√©ponse moyen (TRM)** : Mesure la rapidit√© du traitement

## 4.8 Avancement par rapport √† la planification initiale

Voici un tableau comparatif entre la **planification initiale** et l‚Äô**√©tat actuel du projet** :

| **T√¢che**                                      | **Planification initiale (plan.pdf)**     | **√âtat actuel (report.pdf)**           | **Statut**  |
|-----------------------------------------------|-----------------------------------------|--------------------------------------|------------|
| **Mise en place de l‚Äôenvironnement**         | 5h ‚Äì Environnement Python, GitHub      | Environnement configur√©, GitHub pr√™t, Black & Pylint utilis√©s | ‚úÖ Termin√©  |
| **Pr√©paration de la base de donn√©es**        | 5h ‚Äì Cr√©ation d‚Äôune base DuckDB        | Base DuckDB cr√©√©e avec filtrage des produits canadiens | ‚úÖ Termin√©  |
| **Cr√©ation du jeu de test (100 questions)**  | 10h ‚Äì G√©n√©rer des questions et requ√™tes SQL | 100 questions-r√©ponses cr√©√©es en fran√ßais et anglais | ‚úÖ Termin√©  |
| **Documentation des donn√©es**                | Pr√©voir un dictionnaire des colonnes   | Documentation d√©taill√©e (109 colonnes, exemples, SQL) | ‚úÖ Termin√©  |
| **D√©veloppement de la conversion texte-SQL** | 25h ‚Äì Approche texte-SQL avec LLM      | Impl√©mentation avec FAISS pour recherche s√©mantique | ‚úÖ Termin√©  |
| **D√©veloppement du module de dialogue**      | 20h ‚Äì Utilisation de Qwen2-7B-Instruct | Utilisation de Llama3.1:8B, Qwen2.5:7B, Claude 3.5 | ‚úÖ Termin√©  |
| **Impl√©mentation du g√©n√©rateur de r√©ponses** | 25h ‚Äì Transformer r√©sultats SQL en texte | Fonctionnel avec SmolAgents et int√©gration Guide alimentaire canadien | ‚úÖ Termin√©  |
| **Strat√©gie d‚Äô√©valuation (m√©triques EX, PS, RS, TRM)** | 10h ‚Äì D√©finir et tester les m√©triques | M√©triques d√©finies et tests r√©alis√©s sur plusieurs mod√®les | ‚úÖ Termin√©  |
| **Optimisation des requ√™tes SQL et prompts** | 15h ‚Äì Am√©liorer la g√©n√©ration SQL      | Ajustements des prompts et meilleure gestion des colonnes SQL | ‚úÖ Termin√©  |
| **Gestion des donn√©es manquantes**           | 15h ‚Äì Trouver des strat√©gies alternatives | Approche hybride avec Guide alimentaire canadien int√©gr√©e | ‚úÖ Termin√©  |
| **S√©lection dynamique de LLM**               | 10h ‚Äì Tester plusieurs mod√®les pour √©quilibre co√ªt/performance | Tests r√©alis√©s sur plusieurs mod√®les, r√©sultats document√©s | ‚úÖ Termin√©  |
| **Exploration de nouvelles m√©thodes (ex. RAG, Neo4j)** | Optionnelle en phase avanc√©e          | Mentionn√©e comme piste future | ‚è≥ √Ä explorer |
| **Finalisation et rapport final**            | Rapport mi-session : 15h, final : 15h  | Rapport mi-session r√©dig√©, reste le rapport final | ‚è≥ En cours |

üîπ **R√©sum√© :**  
Toutes les t√¢ches **principales sont termin√©es** selon la planification. Les optimisations avanc√©es et l‚Äôexploration de nouvelles approches (ex. RAG) sont **√† explorer**. La **r√©daction du rapport final** est en cours.


## 5. R√©sultats et discussion

Le tableau suivant r√©sume les r√©sultats des tests effectu√©s sur l'agent conversationnel en utilisant diff√©rents mod√®les de langage et en √©valuant les m√©triques EX, PS, RS et TRM
sur 20 questions pour chaque mod√®le, en fran√ßais et en anglais.



|                          |Llama3.1:8b-Instruct (en)|Qwen2.5:7b-Instruct (en)|Claude 3.5 Sonnet (en)|Llama3.1:8b-Instruct (fr)|Qwen2.5:7b-Instruct (fr)|Claude 3.5 Sonnet (fr)| 
|--------------------------|---:|---:|---:|---:|---:|---:|
|Nombre de questions       |20|20|20|20|20|20|
|Pr√©cision d'ex√©cution (EX) (%)             |31.1|28.6|42.5|25.0|40.0|47.5|
|Pr√©cision s√©mantique (PS) (%)      |16.7|66.5|62.3|20.0|49.5|54.7|
|Respect de s√©quence (RS) (%)       |100.0|100.0|100.0|100.0|100.0|100.0|
|Temps de r√©ponse moyen (TRM)|432s|239s|37s|273s|201|41s|

Ici Llama3.1:8b-Instruct r√©f√®re au mod√®le `ollama/llama3.1:8b-instruct-q8_0`, Qwen2.5:7b-Instruct au mod√®le `ollama/qwen2.5:7b-instruct` et Claude 3.5 Sonnet au mod√®le `anthropic/claude-3-5-sonnet-20241022`. Les tests ont √©t√© r√©alis√©s sur un MacBook Pro M1 16 Go de RAM.

Les principales observations sont les suivantes :

- **Performance des mod√®les** : Claude 3.5 Sonnet surpasse significativement les mod√®les Llama3.1 et Qwen2.5 sur presque toutes les m√©triques, particuli√®rement en pr√©cision d'ex√©cution (EX) et en temps de r√©ponse moyen (TRM).
- **Diff√©rences linguistiques** : Les performances varient selon la langue. Les r√©sultats en fran√ßais montrent g√©n√©ralement une bonne pr√©cision d'ex√©cution pour Claude et Qwen par rapport √† l'anglais, sugg√©rant une bonne capacit√© de traitement multilingue.
- **Respect de s√©quence** : Tous les mod√®les obtiennent un score parfait (100%) sur le respect de s√©quence (RS), indiquant que la strat√©gie de recherche d√©finie est bien suivie ind√©pendamment du mod√®le utilis√©.
- **Pr√©cision s√©mantique** : Qwen2.5 surpasse remarquablement Llama3.1 en pr√©cision s√©mantique, particuli√®rement en anglais (66.5% contre 16.7%), sugg√©rant peut-√™tre une meilleure compr√©hension du domaine des produits alimentaires.

Ces r√©sultats d√©montrent clairement l'avantage d'utiliser un mod√®le commercial comme Claude 3.5 Sonnet pour des applications pratiques, bien que Qwen2.5 offre une alternative gratuite int√©ressante avec des performances acceptables, particuli√®rement pour la compr√©hension s√©mantique.

Ces √©valuations s'effectuent dans le script `evaluation_04.py`.

## 6. Probl√®mes rencontr√©s et solutions

### 6.1 Complexit√© structurelle des donn√©es

Un d√©fi majeur a √©t√© de comprendre les structures complexes d'Open Food Facts. Sans documentation officielle compl√®te, j'ai d√ª explorer la base en d√©tail pour comprendre les relations entre les colonnes.

La base contient des structures h√©t√©rog√®nes, comme les colonnes "categories" (texte libre avec virgules) et "categories_tags" (tableaux avec pr√©fixes de langue). Cette complexit√© rend difficile la g√©n√©ration de bonnes requ√™tes SQL.

Pour r√©soudre ce probl√®me, j'ai cr√©√© une documentation d√©taill√©e avec :

- Le type de donn√©es et sa description
- Des exemples de valeurs
- Des mod√®les de requ√™tes SQL adapt√©es

Je pense qu'on peut encore am√©liorer cette documentation, peut-√™tre avec plus d'exemples annot√©s.

### 6.2 Limitations des mod√®les de langage l√©gers

L'utilisation de mod√®les gratuits (ex. `ollama/llama3.1:8b-instruct-q8_0`) pendant le d√©veloppement a permis d'avancer sans co√ªts d'API, mais a montr√© des limites :

- Difficult√© √† comprendre des instructions complexes
- Probl√®mes pour maintenir un format de r√©ponse coh√©rent
- G√©n√©ration SQL parfois incorrecte
- Difficult√© √† suivre des s√©quences d'actions

Pour r√©soudre ces probl√®mes, j'ai :

- Am√©lior√© les prompts: Instructions plus simples et directes, avec plus d'exemples
- Modifi√© l'architecture: Utilis√© les m√©canismes de MultiStepAgent de Smolagents pour mieux suivre la progression

Des tests avec `anthropic/claude-3-5-sonnet` montrent des r√©sultats bien meilleurs, confirmant l'int√©r√™t d'une approche hybride: d√©veloppement avec des mod√®les l√©gers, puis d√©ploiement avec des mod√®les plus puissants.

## 7. Prochaines √©tapes

Si je continue ce projet sur cette voie, je peux :

- **Mieux documenter la structure de la base de donn√©es** : Cr√©er une meilleure carte des relations entre colonnes, ajouter des exemples pour les structures difficiles, et enrichir la documentation avec des informations sur la distribution des donn√©es.
- **Optimiser la recherche s√©mantique** : Ajouter une recherche vectorielle sur les fiches produits en plus de l'approche SQL. Cela aiderait √† trouver des produits similaires quand les requ√™tes SQL sont limit√©es.

Je pourrais aussi explorer une approche diff√©rente avec **RAG sur un graphe de connaissances** en utilisant Neo4j et Langchain. Cette m√©thode 
pourrait mieux repr√©senter les relations complexes entre produits et informations nutritionnelles.

## 8. R√©f√©rences

Gao, D., Wang, H., Li, Y., Sun, X., Qian, Y., Ding, B., & Zhou, J. (2023). Text-to-SQL Empowered by Large Language Models: A Benchmark Evaluation. *arXiv preprint arXiv:2308.15363*. https://arxiv.org/abs/2308.15363

## Annexe A: Exemple d'√©valuation d'une requ√™te utilisateur

**Question utilisateur** : "What food products without additives are available in the database?"

### √âtape 1: D√©finition de la question et r√©f√©rence dans le jeu de test

Cette question correspond √† la premi√®re entr√©e dans le fichier `qa_pairs.json`, structur√©e comme suit:

```txt
[
  {
    "column": "additives_n",
    "sql": "SELECT code, product_name FROM products WHERE additives_n = 0",
    "questions": {
      "fr": "Quels sont les produits alimentaires sans additifs disponibles dans la base de donn√©es?",
      "en": "What food products without additives are available in the database?"
    },
    "answers": {
      "fr": """La base de donn√©es contient 5843 produits sans additifs, incluant des produits comme 
               le sirop d'√©rable biologique du Vermont, le lait faible en gras, l'agave bleu biologique, 
               et le lait de coco.""",
      "en": """The database contains 5843 products without additives, including items such as organic 
               Vermont maple syrup, low-fat milk, organic blue agave, and coconut milk."""
    }
  },
  ...
]
```

### √âtape 2: Analyse s√©mantique des colonnes pertinentes

Le syst√®me utilise FAISS pour comparer l'embedding de la question avec ceux des descriptions de colonnes :

```python
relevant_columns = self._search_relevant_columns(question)
```

Cette recherche identifie 5 colonnes potentiellement pertinentes avec leurs scores de similarit√© :
- `unknown_ingredients_n` (score : 0,656)
- `additives_tags` (score : 0,638)
- `ingredients_original_tags` (score : 0,622)
- `ingredients_without_ciqual_codes` (score : 0,612)
- `data_quality_info_tags` (score : 0,563)

### √âtape 3: Pr√©paration du prompt pour l'agent

Les informations sur ces colonnes sont pr√©par√©es et stock√©es dans `columns_info` :

```python
columns_info = []
for col in relevant_columns:
    if col['similarity'] > 0.5:
        column_section = [
            f"Column: {col['name']}\n"
            f"Type: {col['type']}\n"
            f"Description: {col['description']}\n"
            f"Examples of values: {', '.join(map(str, col['examples'][:3]))}"
        ]
        
        if 'common_queries' in col and col['common_queries']:
            column_section.append(f"Query examples:")
            for query in col['common_queries']:
                column_section.append(
                    f"# {query.get('description', '')}:\n"
                    f"{query.get('sql', '')}"
                )
        
        columns_info.append("\n".join(column_section))
```

Ces informations sur les colonnes pertinentes sont ins√©r√©es des notes additionnelles pour l'agent
comme ceci :

```python
additional_notes = dedent(
    """\
    You are a helpful assistant that answers questions about food products 
    using the Open Food Facts database.

    POTENTIALLY RELEVANT COLUMNS:
    The following columns have been identified through semantic search as potentially relevant, 
    with their similarity scores (higher means more likely relevant):
    
    {columns_text}

    SEARCH SEQUENCE RULES:
    1. ALWAYS start with database queries using the most relevant columns
    2. If initial query fails, try alternative database queries with different columns or approaches
    3. Only if database queries are unsuccessful, search the Canada Food Guide
    4. Document EVERY attempt in the steps array, including failures
    5. Never skip straight to Food Guide without trying database first
    6. Always include the source of the information in the answer ("Open Food Facts" or "Canada Food Guide")
    7. Always respond in the same language as the question (French or English)
    
    RESPONSE FORMAT REQUIREMENTS:
    1. Provide ONLY the natural language answer to the user's question
    2. Maximum response length: 200 characters
    3. DO NOT include SQL queries, code snippets, or technical details
    4. DO NOT explain your reasoning or methodology
    5. Respond in the same language as the question (French or English)
    6. DO mention the source of information ("Open Food Facts" or "Canada Food Guide")
    
    Please follow these rules to ensure a consistent and effective search strategy.
    """
).format(columns_text=columns_text)
```

### √âtape 4: Ex√©cution de l'agent

L'agent est appel√© avec le prompt enrichi :

```python
start_time = time.time()
agent_response = agent.run(
    question,
    additional_args={"additional_notes": additional_notes},
)
response_time = time.time() - start_time
```

### √âtape 5: G√©n√©ration de la r√©ponse

L'agent analyse les informations et g√©n√®re une r√©ponse :

"According to Open Food Facts database, additive-free products include natural 
foods like blueberries and pistachios, basic staples like spaghetti and rice, 
and beverages like coconut water and coffee."

### √âtape 6: Tra√ßage des √©tapes de l'agent

Le syst√®me enregistre toutes les √©tapes suivies par l'agent :

```python
# Extrait les √©tapes depuis la m√©moire de l'agent
if hasattr(agent, 'memory') and hasattr(agent.memory, 'steps'):
    steps_sequence = []
    
    for step in agent.memory.steps:
        # Analyse chaque √©tape et outil utilis√©
        if not hasattr(step, 'tool_calls') or not step.tool_calls:
            continue
            
        tool_call = step.tool_calls[0]
        
        # Enregistre les requ√™tes SQL, recherches web et autres actions
        if tool_call.name == "python_interpreter" and isinstance(tool_call.arguments, str):
            code = tool_call.arguments
            step_data = {}
            
            # D√©termine le type d'action (requ√™te SQL, recherche web, etc.)
            if "query_db" in code:
                sql_query = self._extract_sql_from_code(code)
                # Enregistre l'information sur la requ√™te SQL
            elif "search_food_guide" in code:
                # Enregistre l'information sur la recherche web
```

Ces √©tapes permettent de suivre la s√©quence de recherche (d'abord la base de donn√©es, puis le guide alimentaire si n√©cessaire).

### √âtape 7: √âvaluation des m√©triques

Trois m√©triques principales sont calcul√©es :

**1. Pr√©cision d'ex√©cution (EX)** : Comparer la requ√™te g√©n√©r√©e avec la r√©f√©rence

```python
def _calculate_sql_accuracy(self, response_data: dict, qa_pair: Dict[str, Any]) -> float:
    agent_sql = response_data.get('sql_query')
    
    # Ex√©cuter les requ√™tes
    reference_results = self.execute_query(qa_pair['sql'])
    agent_results = self.execute_query(agent_sql)

    # Calculer les m√©triques individuelles
    query_present = 1.0 if agent_sql else 0.0
    execution_success = float(agent_results.success)
    results_match = 0.0

    # Comparer les r√©sultats
    if reference_results.success and agent_results.success:
        # Calcul de la similarit√© entre les deux ensembles de r√©sultats
```

**2. Pr√©cision s√©mantique (PS) : Comparer la r√©ponse g√©n√©r√©e avec la r√©f√©rence**

Je demande √† un mod√®le LLM de comparer les deux r√©ponses pour √©valuer leur similarit√© s√©mantique.

Les deux r√©ponses sont : 

- **R√©ponse attendue** : "The database contains 5843 products without additives, including items such as organic Vermont maple syrup, low-fat milk, organic blue agave, and coconut milk."
- **R√©ponse de l'agent** : "According to Open Food Facts database, additive-free products include natural foods like blueberries and pistachios, basic staples like spaghetti and rice, and beverages like coconut water and coffee."

```python
def _calculate_semantic_accuracy(self, response_data: dict, qa_pair: Dict, lang: str) -> float:
    # Utiliser un LLM pour √©valuer la similarit√© s√©mantique
    prompt = dedent(f"""\
    Compare these two responses and rate their semantic similarity from 0 to 1:
    Response #1: {qa_pair['answers'][lang]}
    Response #2: {agent_response}
    """)
    
    # Le mod√®le LLM √©value et retourne un score de similarit√©
```

**3. Respect de s√©quence (RS)** : V√©rifier que l'agent a suivi le bon ordre des sources

```python
def _evaluate_search_sequence(self, response_data: dict) -> dict:
    steps = response_data.get('steps', [])
    
    # Initialiser les compteurs
    db_attempts = []
    web_attempt = None
    sequence_respected = True
    
    # V√©rifier l'ordre des √©tapes
    for step in steps:
        if step['action'] in ['database_query', 'alternative_query']:
            db_attempts.append(step)
            # V√©rifier si une recherche web a d√©j√† eu lieu (violation)
            if web_attempt is not None:
                sequence_respected = False
                break
        elif step['action'] == 'food_guide_search':
            web_attempt = step
```

### √âtape 8: Compilation des r√©sultats

Les r√©sultats de l'√©valuation sont compil√©s :

```python
# Cr√©er un objet EvaluationResult contenant toutes les informations
return EvaluationResult(
    question_id=qa_pair.get('id', 0),
    language=lang,
    question=question,
    expected_answer=qa_pair['answers'][lang],
    agent_answer=agent_answer,
    metrics=metrics,
    expected_sql=qa_pair.get('sql', ''),
    agent_sql=agent_sql
)
```

### R√©sultats finaux

L'√©valuation finale montre :

- Pr√©cision d'ex√©cution (EX) : 0,00% (l'agent n'a pas g√©n√©r√© de requ√™te SQL valide)
- Pr√©cision s√©mantique (PS) : 80,00% (forte similarit√© entre les r√©ponses)
- Respect de s√©quence (RS) : 100,00% (protocole de recherche respect√©)
- Temps de r√©ponse moyen (TR) : 45,95 secondes

Cet exemple illustre le parcours complet, d√©montrant comment l'agent analyse la question, g√©n√®re une r√©ponse et comment le syst√®me
√©value objectivement cette r√©ponse selon plusieurs dimensions.
